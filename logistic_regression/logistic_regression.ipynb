{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a903568b-3359-425c-aeb6-bff86477e512",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "1. From linear combinations to predicting the probability.\n",
    "2. How to find coefficients $\\theta$? Maximum likelihood.\n",
    "3. Gradient descent implementation\n",
    "4. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bcfeb-5bc0-4675-bcaa-26adc9462f87",
   "metadata": {},
   "source": [
    "### Linear combinations >> probability\n",
    "\n",
    "The idea is similar to Linear Regression - use $X*\\theta$ for prediction. But now we say that we want to predict the probability - the value $P(y | X) \\in [0 ... 1]$\n",
    "\n",
    "$$\n",
    "P(y = 1|x;\\theta) = h_\\theta(x) \\\\\n",
    "P(y = 0|x;\\theta) = 1 - h_\\theta(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{We need some }g(\\theta^T * X)\\\\\n",
    "h_\\theta(X) = g(\\theta^T * X) = g(z), \\text{ where } z = \\theta^T * X \\in [-\\infty ... +\\infty]\\\\\n",
    "\\text {In other words, we need:} \\\\\n",
    "\\theta^T * X \\in [-\\infty ... +\\infty] =>> P(y | X) \\in [0 ... 1]\n",
    "$$\n",
    "\n",
    "- transform $\\theta^T * X$ into $[0 ... 1]$\n",
    "- or find some $f(P)$ that returns $[-\\infty ... +\\infty]$ and derive $P(y | X)$ from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27330f35-ccf7-4608-8393-c393cd196524",
   "metadata": {},
   "source": [
    "### Odds and log(odds)\n",
    "\n",
    "$$\n",
    "P(y = 1|x;\\theta) = h_\\theta(x) \\in [0 ... 1]\\\\\n",
    "Odds(y = 1) = \\frac {P(y = 1)} {P(y = 0)} = \\frac {P(y = 1)} {1 - P(y = 1)}  \\in [0 ... +\\infty] \\\\\n",
    "\\log{Odds(y = 1)} = \\log{\\frac {P(y = 1)} {1 - P(y = 1)}} \\in [-\\infty ... +\\infty] \\\\\n",
    "\\text {So, here we can say that} \\\\\n",
    "\\theta^T * X = \\log{\\frac {P(y = 1)} {1 - P(y = 1)}} \\\\ \n",
    "\\text{Then:} \\\\\n",
    "h_\\theta(X) = \\frac {1} {1 + e^{-(\\theta^T * X)}}\n",
    "$$\n",
    "\n",
    "Now the question is - how to find the best $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635e0d2-e48f-44fa-b57e-3bb1e7b54bee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{For } y = 1: p(y|x,\\theta) = h_\\theta(x) \\\\\n",
    "\\text{For } y = 0: p(y|x,\\theta) = 1 - h_\\theta(x) \\\\\n",
    "\\text{Same thing with one formula:} \\\\\n",
    "p(y|x,\\theta) = (h_\\theta(x))^y (1 - h_\\theta(x))^{1 - y} \\\\ \n",
    "p(y|x,\\theta) = (\\frac {1} {1 + e^{-(\\theta^T * X)}})^y (1 - \\frac {1} {1 + e^{-(\\theta^T * X)}})^{1 - y}\n",
    "$$\n",
    "\n",
    "Now the LIKELIHOOD of parametrs $\\theta$ is basically the same as the probability of y but with $\\theta$ as a variable\n",
    "\n",
    "$$\n",
    "L(\\theta) = p(\\vec{y}|X;\\theta) \\\\\n",
    "= \\prod_{i=1}^{n} p(y^{(i)}|x^{(i)};\\theta) \\\\\n",
    "= \\prod_{i=1}^{n} (h_\\theta(x^{(i)}))^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{1 - y^{(i)}} \\\\\n",
    "= \\prod_{i=1}^{n} (\\frac {1} {1 + e^{-(\\theta * x^{(i)})}})^y (1 - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}})^{1 - y}\n",
    "$$\n",
    "\n",
    "As with Linear Regression, we replace Linelihood with LOG of Likelihood. Then PRODUCT will transform to SUMM.\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\log L(\\theta) \\\\\n",
    "= \\sum_{i=1}^n y^{(i)} \\log h(x^{(i)}) + (1 - y^{(i)})log(1 - h(x^{(i)}) \\\\\n",
    "= \\sum_{i=1}^n y^{(i)} \\log \\frac {1} {1 + e^{-(\\theta * x^{(i)})}} + (1 - y^{(i)})log(1 - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}}) \\\\\n",
    "$$\n",
    "\n",
    "Logistic function is monotonic, which allows us to maximize $\\log (loss)$. But now the value depends on the dataset size - sum from 1 to n. So, for convenience we need to scale the whole equation:\n",
    "\n",
    "$$\n",
    "l(\\theta) = \\frac{1}{n} * \\sum_{i=1}^n y^{(i)} \\log \\frac {1} {1 + e^{-(\\theta * x^{(i)})}} + (1 - y^{(i)})log(1 - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}}) \\\\\n",
    "$$\n",
    "\n",
    "And this thing we want to maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8938ccb-4344-4ed2-9f99-a05376e732ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Derivative of Likelihood function\n",
    "\n",
    "In advance we can say:\n",
    "\n",
    "$$\n",
    "\\text{1. } \\frac {\\partial{}} {\\partial{\\theta}} h_\\theta(X) = h_\\theta(X) * (1 - h_\\theta(X)) * X = \\frac {1} {1 + e^{-(\\theta^T * X)}} * (1 - \\frac {1} {1 + e^{-(\\theta^T * X)}}) * X \\\\\n",
    "\\text{2. } \\frac {\\partial{}} {\\partial{\\theta}} \\log{h_\\theta(X)} = \\frac{1}{h_\\theta(X)} * h_\\theta(X) * (1 - h_\\theta(X) * X) = (1 - h_\\theta(X)) * X \\\\\n",
    "\\text{3. } \\frac {\\partial{}} {\\partial{\\theta}} \\log{\\left(1 - h_\\theta(X) \\right)} = \\frac{1}{1 - h_\\theta(X)} * \\left[ 0 - h_\\theta(X) * (1 - h_\\theta(X)) * X \\right]  =  \\\\\n",
    "= \\frac{1}{1 - h_\\theta(X)} * 0 - \\frac{1}{1 - h_\\theta(X)} * h_\\theta(X) * (1 - h_\\theta(X)) * X = \\\\\n",
    "= - h_\\theta(X) * X\\\\\n",
    "$$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial{}} {\\partial{\\theta}} l(\\theta) = \\frac{1}{n} * \\sum_{i = 1}^{n} X * \\left( y^{(i)} - h_\\theta(X) \\right) = \\frac{1}{n} * \\sum_{i = 1}^{n} X * \\left( y^{(i)} - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}} \\right)\n",
    "$$\n",
    "\n",
    "Gradient solution:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta + \\alpha * \\frac {\\partial{}} {\\partial{\\theta}} l(\\theta) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f05464-ecfe-45db-881f-f09061755284",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Works the same as for Linear Regression:\n",
    "\n",
    "$$\n",
    "\\text{old_l}(\\theta) = \\frac{1}{n} * \\sum_{i=1}^n y^{(i)} \\log \\frac {1} {1 + e^{-(\\theta * x^{(i)})}} + (1 - y^{(i)})log(1 - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}}) \\\\\n",
    "\\text{new_l}(\\theta) = \\text{old_l}(\\theta) + \\lambda * ||\\theta||_k^k\n",
    "$$\n",
    "\n",
    "For k=2:\n",
    "$$\n",
    "\\text{new_l}(\\theta) = \\text{old_l}(\\theta) + \\lambda * ||\\theta||_2^2 \\\\\n",
    "\\frac {\\partial{}} {\\partial{\\theta}} \\text{new_l}(\\theta) = \\frac {\\partial{}} {\\partial{\\theta}} \\text{old_l}(\\theta) + 2 * \\lambda * \\theta = \\\\\n",
    "= \\frac{1}{n} * \\sum_{i = 1}^{n} X * \\left( y^{(i)} - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}} \\right) + 2 * \\lambda * \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta := \\theta + \\alpha * \\frac {\\partial{}} {\\partial{\\theta}} \\text{new_l}(\\theta) = \\\\\n",
    "\\theta := \\theta + \\alpha * \\left( \\frac{1}{n} * \\sum_{i = 1}^{n} X * \\left( y^{(i)} - \\frac {1} {1 + e^{-(\\theta * x^{(i)})}} \\right) + 2 * \\lambda * \\theta \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c5727-4968-4c6f-9e64-ee79769d4b54",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d68aa1-e2eb-450a-9689-ded445513e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58332e86-a2ff-4eac-8006-11799a81107e",
   "metadata": {},
   "source": [
    "I'll be using Spaceship Titanic dataset from [Kaggle competition](https://www.kaggle.com/competitions/spaceship-titanic)\n",
    "\n",
    "Data description:\n",
    "* PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "* HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "* CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "* Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "* Destination - The planet the passenger will be debarking to.\n",
    "* Age - The age of the passenger.\n",
    "* VIP - Whether the passenger has paid for special VIP service during the voyage.\n",
    "* RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "* Name - The first and last names of the passenger.\n",
    "* Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36140b7f-43cb-4cc9-a906-33eef696cabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/spaceship_titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c757d1-7309-4be9-83ac-13a4cf61ac81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce745f1-eaf6-48e4-9084-1ffddcd00741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data[['Age', 'FoodCourt', 'ShoppingMall', 'VRDeck', 'Spa']]\n",
    "X = X.dropna()\n",
    "y = data['Transported']\n",
    "y = y[X.index].astype('int').values\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "ones = np.ones(len(X))[:, None]\n",
    "X = np.concatenate([ones, X], axis=1)\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106283d0-f7ea-47c9-8e50-bd28c7a0a6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogRegression:\n",
    "    def __init__(self, alpha=0.01, n_iterations=1000):\n",
    "        self.alpha = alpha\n",
    "        self.n_iterations = n_iterations\n",
    "        return\n",
    "    \n",
    "    def hypothesis(self, theta, X):\n",
    "        return 1 / (1 + np.exp(-theta @ X.T))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        hypothesis = self.hypothesis(self.theta, X)\n",
    "        errors = y - hypothesis\n",
    "        grad = X.T @ errors\n",
    "        self.theta += self.alpha * grad\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            hypothesis = self.hypothesis(self.theta, X)\n",
    "            errors = y - hypothesis\n",
    "            grad = X.T @ errors / len(X)\n",
    "            self.theta += self.alpha * grad\n",
    "        return self.theta\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.hypothesis(self.theta, X)\n",
    "    \n",
    "    \n",
    "model = LogRegression(alpha=0.01, n_iterations=1000)\n",
    "thetas = model.fit(X, y)\n",
    "predictions_proba = model.predict(X)\n",
    "predictions = np.where(predictions_proba > 0.5, 1, 0)\n",
    "\n",
    "print(f'Thetas: {thetas}')\n",
    "print(f'Accuracy: {balanced_accuracy_score(y, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2536cf6-57f8-4cec-8278-50467ce9167e",
   "metadata": {},
   "source": [
    "With regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178181e4-4023-4f0d-a741-7dbd8dcc9080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogRegressionL2:\n",
    "    def __init__(self, alpha=0.01, n_iterations=1000, c=0):\n",
    "        self.alpha = alpha\n",
    "        self.n_iterations = n_iterations\n",
    "        self.lmbd = c\n",
    "        return\n",
    "    \n",
    "    def hypothesis(self, theta, X):\n",
    "        return 1 / (1 + np.exp(-theta @ X.T))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        hypothesis = self.hypothesis(self.theta, X)\n",
    "        errors = y - hypothesis\n",
    "        grad = X.T @ errors\n",
    "        self.theta += self.alpha * grad\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            dummy_theta = self.theta.copy()\n",
    "            dummy_theta[0] = 0\n",
    "            hypothesis = self.hypothesis(self.theta, X)\n",
    "            errors = y - hypothesis\n",
    "            grad = X.T @ errors / len(X)\n",
    "            self.theta += self.alpha * (grad + self.lmbd * dummy_theta)\n",
    "        return self.theta\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.hypothesis(self.theta, X)\n",
    "    \n",
    "    \n",
    "model = LogRegressionL2(alpha=0.01, n_iterations=1000, c=0.01)\n",
    "thetas = model.fit(X, y)\n",
    "predictions_proba = model.predict(X)\n",
    "predictions = np.where(predictions_proba > 0.5, 1, 0)\n",
    "\n",
    "print(f'Thetas: {thetas}')\n",
    "print(f'Accuracy: {balanced_accuracy_score(y, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c622710-32fd-4d80-b7b5-355a1db6c52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "a[0] = 123\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aabfef-4118-46aa-823d-d04e869ba285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
